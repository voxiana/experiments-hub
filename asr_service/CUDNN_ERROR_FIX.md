# cuDNN Error Fix - ASR Service

## ‚ùå The Error

```
2025-11-11 07:48:10,966 - faster_whisper - INFO - Processing audio with duration 00:06.454
Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so}
Invalid handle. Cannot load symbol cudnnCreateTensorDescriptor
```

---

## üîç Root Cause

**faster-whisper** (and its dependency **CTranslate2**) requires **cuDNN 9**, but the Docker image was using **cuDNN 8**.

### Version Mismatch:
- **Original Dockerfile**: `nvidia/cuda:12.1.0-cudnn8-...` ‚ùå
- **Required**: `nvidia/cuda:12.4.0-cudnn9-...` ‚úÖ

The error message shows it's looking for `libcudnn_ops.so.9.*` files, which are only present in cuDNN 9.

---

## ‚úÖ Solution

Updated both the Dockerfile and requirements.txt to use **CUDA 12.4 with cuDNN 9**.

### Changes Made:

#### 1. **Dockerfile - Base Images**

**Before (broken):**
```dockerfile
FROM nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04 as builder
FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04
```

**After (fixed):**
```dockerfile
FROM nvidia/cuda:12.4.0-cudnn9-devel-ubuntu22.04 as builder
FROM nvidia/cuda:12.4.0-cudnn9-runtime-ubuntu22.04
```

#### 2. **requirements.txt - PyTorch Version**

**Before:**
```python
torch==2.1.1+cu121
torchaudio==2.1.1+cu121
--index-url https://download.pytorch.org/whl/cu121
```

**After:**
```python
torch==2.2.0+cu124
torchaudio==2.2.0+cu124
--index-url https://download.pytorch.org/whl/cu124
```

#### 3. **Added Environment Variables**

Added to Dockerfile to help with library loading:
```dockerfile
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
ENV CUDA_MODULE_LOADING=LAZY
```

---

## üß™ Verification

### Step 1: Rebuild Docker Image

```bash
cd asr_service

# Clean old image
docker rmi voiceai-asr:latest

# Rebuild with cuDNN 9
docker build -f Dockerfile -t voiceai-asr:latest .
```

### Step 2: Run Container

```bash
docker run --gpus all -p 8050:8050 \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  voiceai-asr:latest
```

### Step 3: Test Transcription

```bash
# Generate test audio
python test_audio.py --output test.wav --duration 5

# Test transcription
curl -F "file=@test.wav" http://localhost:8050/transcribe
```

**Expected Output:**
```json
{
  "text": "...",
  "language": "en",
  "language_probability": 0.95,
  "duration_seconds": 5.0,
  "inference_time_seconds": 0.42
}
```

**No cuDNN errors!** ‚úÖ

---

## üîç How to Check cuDNN Version

### Inside Docker Container:

```bash
# Enter running container
docker exec -it <container_id> bash

# Check cuDNN version
cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2

# Should output:
# #define CUDNN_MAJOR 9
# #define CUDNN_MINOR 1
# #define CUDNN_PATCHLEVEL 0
```

### From Python:

```python
import torch
print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"CUDA Version: {torch.version.cuda}")
print(f"cuDNN Version: {torch.backends.cudnn.version()}")

# Should output:
# CUDA Available: True
# CUDA Version: 12.4
# cuDNN Version: 90100  (9.1.0)
```

---

## üéØ Why This Happened

### CTranslate2 Prebuilt Binaries

**faster-whisper** uses **CTranslate2** which is distributed as prebuilt binaries. These binaries are compiled against specific cuDNN versions.

As of late 2024, CTranslate2 binaries require:
- **cuDNN 9.x** (not 8.x)
- **CUDA 12.x** (preferably 12.4)

### Version Compatibility Matrix:

| Component | Version | cuDNN Required |
|-----------|---------|----------------|
| CTranslate2 | 3.24.0 | 9.1.0 ‚úÖ |
| faster-whisper | 0.10.0 | 9.1.0 ‚úÖ |
| PyTorch | 2.2.0+cu124 | 9.1.0 ‚úÖ |
| CUDA Base Image | 12.4.0-cudnn9 | 9.1.0 ‚úÖ |

---

## üîÑ Alternative Solutions (Not Recommended)

If you absolutely need to use cuDNN 8, you would need to:

### Option 1: Build CTranslate2 from Source

```dockerfile
# Install cuDNN 8 dev libraries
RUN apt-get install -y libcudnn8-dev

# Build CTranslate2 from source
RUN git clone https://github.com/OpenNMT/CTranslate2.git && \
    cd CTranslate2 && \
    mkdir build && cd build && \
    cmake .. -DCMAKE_CUDA_ARCHITECTURES=80 && \
    make -j4 && make install

# Install faster-whisper without binary dependencies
RUN pip install --no-binary :all: faster-whisper
```

**Cons:**
- Much longer build time (30+ minutes)
- Larger image size
- More complex to maintain
- May have compatibility issues

### Option 2: Use Older faster-whisper

```python
# Use an older version that supports cuDNN 8
faster-whisper==0.7.0  # May work with cuDNN 8
```

**Cons:**
- Missing newer features
- Potentially worse performance
- Security vulnerabilities

---

## ‚úÖ Recommended: Use cuDNN 9 (What We Did)

This is the **simplest and most reliable** solution:
- ‚úÖ Use official NVIDIA base images
- ‚úÖ Use prebuilt CTranslate2 binaries
- ‚úÖ Fast build times
- ‚úÖ Smaller image size
- ‚úÖ Better performance
- ‚úÖ Up-to-date with latest features

---

## üìã Checklist - Is cuDNN 9 Installed?

Run these inside your Docker container:

```bash
# 1. Check CUDA libraries path
ls -la /usr/local/cuda/lib64/ | grep cudnn

# Should see files like:
# libcudnn.so.9 -> libcudnn.so.9.1.0
# libcudnn_ops.so.9 -> libcudnn_ops.so.9.1.0  ‚Üê This is what was missing!
# libcudnn_cnn.so.9 -> libcudnn_cnn.so.9.1.0

# 2. Check with ldconfig
ldconfig -p | grep cudnn

# 3. Verify from Python
python3 -c "import ctranslate2; print(ctranslate2.__version__)"
# Should succeed without errors
```

---

## üöÄ Quick Test Script

Save as `test_cudnn.py`:

```python
#!/usr/bin/env python3
"""Test if cuDNN is properly installed"""

import torch
import sys

print("=" * 60)
print("cuDNN Installation Check")
print("=" * 60)

# Check CUDA
cuda_available = torch.cuda.is_available()
print(f"‚úì CUDA Available: {cuda_available}")

if cuda_available:
    print(f"‚úì CUDA Version: {torch.version.cuda}")
    print(f"‚úì cuDNN Version: {torch.backends.cudnn.version()}")
    print(f"‚úì cuDNN Enabled: {torch.backends.cudnn.enabled}")
    print(f"‚úì GPU Count: {torch.cuda.device_count()}")
    print(f"‚úì GPU Name: {torch.cuda.get_device_name(0)}")

    # Check if cuDNN 9
    cudnn_version = torch.backends.cudnn.version()
    major_version = cudnn_version // 1000

    if major_version >= 9:
        print(f"\n‚úÖ SUCCESS: cuDNN {major_version} is installed!")
        sys.exit(0)
    else:
        print(f"\n‚ùå ERROR: cuDNN {major_version} found, but need cuDNN 9+")
        sys.exit(1)
else:
    print("\n‚ùå ERROR: CUDA not available!")
    sys.exit(1)
```

Run inside container:
```bash
python3 test_cudnn.py
```

---

## üìä Before vs After

### Before (cuDNN 8) ‚ùå
```
Unable to load any of {libcudnn_ops.so.9.1.0, ...}
Invalid handle. Cannot load symbol cudnnCreateTensorDescriptor
Process exits with error
```

### After (cuDNN 9) ‚úÖ
```
2025-11-11 08:15:23,456 - faster_whisper - INFO - Processing audio with duration 00:06.454
{
  "text": "This is a test transcription...",
  "language": "en",
  "duration_seconds": 6.454,
  "inference_time_seconds": 0.387
}
Process completes successfully ‚úÖ
```

---

## üîó References

- **NVIDIA CUDA Images**: https://hub.docker.com/r/nvidia/cuda/tags
- **CTranslate2 Installation**: https://opennmt.net/CTranslate2/installation.html
- **faster-whisper**: https://github.com/guillaumekln/faster-whisper
- **cuDNN Release Notes**: https://docs.nvidia.com/deeplearning/cudnn/release-notes/

---

## üìù Summary

| Issue | Root Cause | Solution |
|-------|-----------|----------|
| `libcudnn_ops.so.9` not found | Docker image had cuDNN 8, not 9 | Use CUDA 12.4 + cuDNN 9 base image |
| `cudnnCreateTensorDescriptor` symbol error | CTranslate2 compiled for cuDNN 9 | Update PyTorch to 2.2.0+cu124 |
| Library loading failures | Missing cuDNN paths | Add LD_LIBRARY_PATH env var |

**Status**: ‚úÖ Fixed
**Last Updated**: 2025-11-11
